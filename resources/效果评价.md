


          
查看resources/总结.md文件，了解项目中使用的评价方法和实验结果数据。
        
            
toolName: view_files
            
status: success
          
            
filePath: /home/robbie/code/molei20021/DRS/resources/总结.md
          
查看resources/执行步骤.txt文件，获取项目运行和数据收集的具体操作步骤。
        
            
toolName: view_files
            
status: success
          
            
filePath: /home/robbie/code/molei20021/DRS/resources/执行步骤.txt
          
# DRS方法效果评价的详细步骤

基于DRS项目的设计目标和实现细节，以下是使用实际数据评价该方法效果的完整流程：

## 一、环境准备阶段

### 1. 硬件与软件环境配置
- **硬件要求**：准备5台服务器（1台master，4台worker），与项目实验环境保持一致
- **软件配置**：
  - 操作系统：Ubuntu 22.04
  - Kubernetes版本：v1.23.4
  - Go环境：用于编译Kubernetes源码
  - Python环境：安装torch、flask、gym等依赖
  - Docker：用于容器运行

### 2. 部署Kubernetes集群
```bash
# 编译修改后的Kubernetes源码
cd $GOPATH/kubernetes
make cmd/kube-scheduler

# 初始化Master节点
cd /home/robbie/code/molei20021/DRS/deploy/scripts
./init.sh
./env.sh

# Worker节点加入集群
kubeadm join <master-node-ip>:6443 --token <your-token> --discovery-token-ca-cert-hash <your-cert-hash>

# 部署网络插件
cd /home/robbie/code/molei20021/DRS/deploy/apps
./apply.sh kube-flannel.yaml
```

## 二、DRS系统部署阶段

### 1. 配置与启动监控组件
- 修改每个Worker节点上monitor.py的IP配置
- 在所有Worker节点上启动监控服务：
```bash
cd /home/robbie/code/molei20021/DRS/drs-monitor
./monitor.sh
```

### 2. 配置与启动调度器
- 修改dqn.py中的节点IP地址配置
- 在Master节点上启动DRS调度器：
```bash
cd /home/robbie/code/molei20021/DRS/drs-scheduler
python dqn.py
```

## 三、数据收集阶段

### 1. 部署测试工作负载
根据实验设计，部署三类工作负载进行测试：

**均匀负载测试**：
```bash
# 以1:1:1比例部署三种应用，每20秒部署1个Pod
cd /home/robbie/code/molei20021/DRS/deploy/apps
# 编写脚本循环部署三种应用
for i in {1..30}; do
  ./apply.sh cpu.yaml && sleep 20
  ./apply.sh net.yaml && sleep 20
  ./apply.sh disk.yaml && sleep 20
done
```

**随机负载测试**：
```bash
# 以1:1:1比例部署，但间隔服从N(20,1²)分布
cd /home/robbie/code/molei20021/DRS/deploy/apps
for i in {1..30}; do
  # 生成随机间隔时间
  interval=$(echo "20 + $RANDOM * 0.002 - 1" | bc)
  ./apply.sh cpu.yaml && sleep $interval
  interval=$(echo "20 + $RANDOM * 0.002 - 1" | bc)
  ./apply.sh net.yaml && sleep $interval
  interval=$(echo "20 + $RANDOM * 0.002 - 1" | bc)
  ./apply.sh disk.yaml && sleep $interval
done
```

**CPU密集负载测试**：
```bash
# 以4:1:1比例部署三种应用
cd /home/robbie/code/molei20021/DRS/deploy/apps
for i in {1..24}; do
  if [ $((i % 6)) -eq 0 ]; then
    ./apply.sh net.yaml
  elif [ $((i % 6)) -eq 3 ]; then
    ./apply.sh disk.yaml
  else
    ./apply.sh cpu.yaml
  fi
  sleep 20
done
```

### 2. 采集关键数据
- **调度器决策数据**：
  - 收集reward.log文件中的奖励值数据
  - 记录DQN训练过程中的Q值变化

- **节点资源使用数据**：
  - 从各个Worker节点的node.log获取6项资源指标
  - 记录每个节点的CPU、内存、网络（收/发）、磁盘（读/写）利用率

- **性能指标数据**：
  - 记录Pod调度延迟时间
  - 测量系统CPU开销和通信延迟

## 四、数据处理与分析阶段

### 1. 计算核心评价指标

**(1) 资源利用率计算**
```python
# 集群平均资源利用率计算公式
# 对每个资源类型计算平均值，再求总体平均
def calculate_avg_utilization(resource_data):
    # resource_data格式：{node_id: {"cpu": [...], "mem": [...], "recv": [...], "tran": [...], "read": [...], "write": [...]}}
    total_sum = 0
    count = 0
    for node, resources in resource_data.items():
        for resource_type, values in resources.items():
            total_sum += sum(values) / len(values)  # 单个资源类型平均值
            count += 1
    return total_sum / count if count > 0 else 0
```

**(2) 负载不平衡度计算**
```python
# 负载不平衡度计算公式：各资源利用率的标准差加权和
def calculate_imbalance(resource_data, weights=None):
    if weights is None:
        weights = {"cpu": 1, "mem": 1, "recv": 1, "tran": 1, "read": 1, "write": 1}
    
    imbalance = 0
    # 对每种资源类型计算标准差
    for resource_type, weight in weights.items():
        # 收集所有节点该资源类型的最新值
        values = [resources[resource_type][-1] for node, resources in resource_data.items()]
        std_dev = np.std(values)
        imbalance += weight * std_dev
    return imbalance
```

**(3) 调度延迟与开销计算**
- 调度延迟：记录Pod从提交到绑定成功的时间差
- CPU开销：对比运行DRS前后系统CPU使用率的差异
- 通信延迟：测量组件间消息传递的时间开销

### 2. 生成对比数据

为全面评价DRS效果，需要与对比算法进行比较：

**部署对比调度器**：
```bash
# 部署Kube-scheduler
kubectl apply -f /etc/kubernetes/manifests/kube-scheduler.yaml

# 部署Round Robin调度器
# 可以通过修改Kubernetes调度器配置实现

# 部署Random调度器
# 可以通过修改Kubernetes调度器配置实现
```

**收集对比数据**：
对每种调度器重复上述数据收集过程，确保实验条件一致性。

## 五、结果评价与可视化阶段

### 1. 奖励值收敛性分析
- 绘制奖励值随迭代次数的变化曲线
- 验证DRS在不同工作负载下是否能在100轮迭代后收敛
- 比较不同工作负载下的最终稳定奖励值

### 2. 资源利用率与负载均衡性评价
- 计算并展示DRS相比Kube-scheduler的资源利用率提升百分比
- 计算并展示DRS相比Kube-scheduler的负载不平衡度降低倍数
- 分别分析6项资源指标的使用情况，特别是网络和磁盘资源的利用率提升

### 3. 开销与延迟评价
- 量化DRS相比Kube-scheduler的额外CPU开销
- 评估通信延迟对整体调度性能的影响
- 分析调度延迟增加的可接受程度

### 4. 工作负载适配性评价
- 对比DRS在三种不同工作负载下的表现差异
- 分析各调度器在不同资源需求特征下的优势与劣势
- 评估DRS的泛化能力和适应性

## 六、实验结论总结

### 1. 性能提升总结
- 确认DRS相比Kube-scheduler的资源利用率提升是否达到或接近27.29%
- 确认负载不平衡度是否降低到原来的约1/2.90
- 量化多资源感知带来的具体收益

### 2. 系统开销分析
- 评估DRS引入的CPU开销（约3.27%）和通信延迟（约0.648%）是否在可接受范围内
- 分析调度延迟增加（约38.2ms vs 4.3ms）对整体应用部署的影响

### 3. 适用场景验证
- 验证DRS在不同工作负载类型下的适应性
- 确认DRS在网络密集型和IO密集型应用场景中的优势
- 分析DRS在CPU密集型场景中的表现

## 注意事项

1. **实验环境一致性**：确保不同调度器测试在相同的硬件、软件环境中进行
2. **统计显著性**：每项实验应重复多次，计算平均值和标准差
3. **数据采集频率**：资源监控应保持每秒一次的采集频率，确保数据准确性
4. **干扰隔离**：实验期间应避免其他应用占用集群资源
5. **参数调优**：DQN算法的超参数（如经验池容量300、目标网络更新间隔50轮）应与原设计保持一致

通过以上步骤，可以全面、客观地评价DRS方法在实际环境中的效果，并验证其相比传统调度器的优势和局限性。
        