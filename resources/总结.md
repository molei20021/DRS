# 1. 一段话总结
为解决Kubernetes原生调度器（Kube-scheduler）仅关注CPU和内存、易导致资源碎片化与利用率低的问题，研究团队提出**DRS（基于深度强化学习的Kubernetes调度器）** ，它通过将调度问题建模为**马尔可夫决策过程**，设计包含CPU、内存、网络（收/发速率）、磁盘（读/写速率）6项指标的状态空间、以可调度节点为动作空间、以集群平均资源利用率（AvgUtil）和节点负载不平衡度（Imbalance）为奖励函数，结合**DRS monitor（资源监控）** 和**DRS decision maker（基于DQN算法的决策）** 实现调度；在5节点Kubernetes集群测试中，相比Kube-scheduler，DRS平均提升**27.29%资源利用率**、降低**2.90×负载不平衡度**，仅增加**3.27% CPU开销**和**0.648%通信延迟**，且能适配均匀、随机、CPU密集3类工作负载。


---


# 2. 思维导图（mindmap）
```mindmap
## 研究背景与问题
- Kubernetes地位：云原生容器编排框架，事实标准
- Kube-scheduler缺陷
  - 资源关注局限：仅CPU、内存，忽略网络、磁盘
  - 视角局限：单节点资源，易致集群资源碎片化
  - 性能问题：资源利用率低、节点负载不平衡
- 传统调度算法不足
  - 启发式算法（轮询、First Fit）：固定策略，难适配动态环境
  - 元启发式算法（蚁群、遗传）：设计复杂，依赖专家知识与手动调参
## DRS核心设计
- 调度问题建模：马尔可夫决策过程（MDP）
  - 状态空间：节点6项资源指标（CPU/内存利用率、网络收/发速率、磁盘读/写速率）+Pod资源需求（归一化至[0,100]）
  - 动作空间：集群内所有可调度节点
  - 奖励函数：α×AvgUtil - β×Imbalance（α、β为经验缩放因子）
- 核心组件
  - DRS scheduler：基于Kubernetes调度框架，自定义过滤与评分函数
  - DRS monitor：单节点多线程监控，每1秒采集资源数据，用队列存储计算平均值
  - DRS decision maker：基于DQN算法，经验池容量N=300，每K=50轮迭代更新目标网络参数
## 实验验证
- 实验环境：5节点Kubernetes集群（1主4从，K8s版本v1.23.4，Ubuntu 22.04）
- 测试工作负载：均匀负载（3类服务1:1:1，每20s部署）、随机负载（同比例，间隔N~(20,1²)）、CPU负载（4:1:1，每20s部署）
- 对比算法：Kube-scheduler、Random、Round Robin
- 核心实验结果
  - 资源利用率：较Kube-scheduler提升27.29%
  - 负载不平衡度：较Kube-scheduler降低2.90×
  - 额外开销：CPU 3.27%、通信延迟0.648%
  - 适配性：100轮迭代后收敛，适配3类工作负载
## 相关工作与结论
- 相关工作
  - Kubernetes调度优化：Stratus（批处理作业）、KCSS（多资源指标+TOPSIS算法）、NetMARKS（网络依赖感知）
  - 强化学习调度：Decima（Spark集群GNN特征）、DeepRM（CNN处理资源状态）、RLSK（多集群DQN调度）
- 研究结论
  - DRS优势：多资源感知、自动学习调度策略、低开销高性能
  - 未来方向：新增GPU/FPGA硬件感知、分布式/多层级调度防单点故障
```


---


# 3. 详细总结
## 一、研究背景与现存问题
1. **Kubernetes与调度需求**  
   Kubernetes是当前主流的云原生容器编排框架，支持容器调度、生命周期管理等功能，已成为云原生领域的**事实标准**，被阿里云、亚马逊、微软等主流云厂商采用；而资源调度是Kubernetes集群管理的核心，优质调度算法可提升集群资源利用率、降低企业成本。

2. **Kube-scheduler的核心缺陷**  
   - 资源关注局限：仅以**CPU和内存**为调度依据，忽略网络（数据包收/发速率）、磁盘（读/写速率）等关键资源，导致网络密集型、IO密集型应用运行时节点负载不均。
   - 调度视角局限：仅评估单节点的CPU和内存均衡性，未考虑集群全局资源分布，在大规模集群中易引发**资源碎片化**，降低整体资源利用率。

3. **传统调度算法的不足**  
   | 算法类型       | 代表算法          | 核心问题                                  |
   |----------------|-------------------|-------------------------------------------|
   | 启发式算法     | 轮询（Round Robin）、First Fit | 采用固定参数与策略，无法适配动态集群环境  |
   | 元启发式算法   | 蚁群算法、模拟退火、遗传算法 | 设计流程复杂，依赖专家知识，需反复手动调参|


## 二、DRS的核心设计与实现
### （1）调度问题建模：马尔可夫决策过程（MDP）
DRS将Kubernetes调度问题抽象为MDP五要素（S, A, R, P, ρ₀），关键设计如下：
- **状态空间（S）**：包含两部分  
  ① 节点资源状态：每个节点的6项可测指标，通过DRS monitor采集并计算**几秒内平均值**（公式：Utilᵗⁱ = Σₖ₌₁ᴷ(Utilₖ)ᵗⁱ / K，Utilₖ∈{CPU, Mem, Recv, Tran, Read, Write}）；  
  ② Pod资源需求：根据Pod配置文件与运行历史，转换为上述6项指标的需求值；  
  所有状态值均归一化至**[0, 100]**，形成一维向量。
- **动作空间（A）**：调度器（智能体）的动作是为待调度Pod选择一个可运行节点，即Actionᵗ ∈ {nodeᵢ | i=1,2,...,n}（n为集群节点数）。
- **奖励函数（R）**：衡量调度动作的优劣，核心依赖两个指标：  
  ① 集群平均资源利用率（AvgUtil）：所有节点6项资源利用率的平均值（公式：AvgUtilᵗ = Σᵢ₌₁ⁿΣₖ₌₁ᴷ(Utilₖ)ᵗⁱ / (nK)）；  
  ② 节点负载不平衡度（Imbalance）：各资源利用率的标准差加权和（公式：Imbalanceᵗ = Σₖ₌₁ᴷweightᵏ×std((Utilₖ)ᵗ¹,..., (Utilₖ)ᵗⁿ)）；  
  最终奖励函数：Reward = α×AvgUtil - β×Imbalance（α、β为经验缩放因子，使两项指标量级一致）。

### （2）DRS三大核心组件
| 组件名称               | 功能职责                                                                 | 关键实现细节                                                                 |
|------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------|
| DRS scheduler          | 遵循Kubernetes调度框架，执行调度周期与绑定周期                           | 自定义“过滤”和“评分”阶段逻辑，接收DRS decision maker的调度结果，完成Pod绑定  |
| DRS monitor            | 监控节点资源，生成集群资源全局视图                                       | 每个工作节点部署，多线程采集6项指标（1秒/次），用队列存储数据，查询时返回平均值 |
| DRS decision maker     | 基于DQN算法学习调度策略，输出最优调度动作                                 | 维护容量N=300的经验池，每K=50轮迭代将评估网络参数复制到目标网络，用ε-贪心策略选动作 |

### （3）DQN算法流程
1. 初始化经验池D（容量N=300）与动作价值函数Q（随机权重）；  
2. 接收Pod调度请求后，向DRS monitor获取节点状态，结合Pod需求生成Stateᵗ；  
3. 以ε概率随机选动作，或以1-ε概率选Q值最大的动作（Actionᵗ），返回给DRS scheduler；  
4. 等待Tᵢ=5秒（确保Pod绑定并运行），获取新状态Stateᵗ⁺¹，计算Rewardᵗ；  
5. 将（Stateᵗ, Actionᵗ, Rewardᵗ, Stateᵗ⁺¹）存入经验池，当池满时随机采样训练，通过梯度下降最小化损失；  
6. 每K=50轮迭代，将评估网络参数复制到目标网络，更新策略。


## 三、实验环境与结果
### （1）实验环境配置
- **硬件与软件**：5台虚拟机构建集群（1主4从），Kubernetes版本v1.23.4，操作系统Ubuntu 22.04；主节点（Node0）配置4核AMD R7-4800H、8GB内存，工作节点（Node1-4）配置2-4核CPU、2-4GB内存。
- **测试应用**：3类资源需求不同的微服务，均封装为Docker镜像。

| 应用名称       | 类型         | 功能描述                          | Docker镜像                  | 资源特征                  |
|----------------|--------------|-----------------------------------|-----------------------------|---------------------------|
| Video Scale    | CPU密集型    | 用FFmpeg缩放视频分辨率            | jolyonjian/apps:cpu-1.0     | 消耗分配的全部CPU（500m） |
| Transmission   | 网络密集型    | 向远程服务器发送/接收数据         | jolyonjian/apps:net-1.0     | 网络收/发速率高           |
| Data Write     | IO密集型      | 用dd命令读取磁盘文件并写入副本    | jolyonjian/apps:io-1.0      | 磁盘读/写速率高           |

### （2）工作负载设计
| 工作负载类型 | 服务部署比例 | 部署间隔规则                          | 核心特点                  |
|--------------|--------------|---------------------------------------|---------------------------|
| 均匀负载     | Video:Transmission:Data Write = 1:1:1 | 固定间隔，每20秒部署1个Pod            | 资源需求稳定、均衡        |
| 随机负载     | 1:1:1        | 间隔服从正态分布T~N(20, 1²)秒         | 资源需求波动随机          |
| CPU负载      | 4:1:1        | 固定间隔，每20秒部署1个Pod            | CPU资源需求占比高        |

### （3）核心实验结果
1. **奖励值与收敛性**：DRS在3类工作负载下均能在**100轮迭代后收敛**，稳定后奖励值显著高于Kube-scheduler、Random、Round Robin；其中CPU负载下Kube-scheduler奖励接近DRS（因Kube-scheduler关注CPU），但均匀/随机负载下DRS优势明显。

2. **资源利用率与负载不平衡度**  
   相比Kube-scheduler，DRS的关键性能提升如下：
   - 平均资源利用率：提升**27.29%**（均匀负载下DRS平均利用率47.60%，Kube-scheduler仅33.60%）；  
   - 负载不平衡度：降低**2.90×**（均匀负载下DRS不平衡度0.02，Kube-scheduler为0.08）；  
   - 多资源覆盖优势：DRS的网络（收/发速率）、磁盘（读/写速率）利用率显著高于Kube-scheduler（如均匀负载下DRS磁盘写速率利用率41.82%，Kube-scheduler仅20.34%）。

3. **开销与延迟**  
   | 指标               | 数值       | 说明                                  |
   |--------------------|------------|---------------------------------------|
   | CPU开销            | 3.27%      | DRS monitor运行导致，仅比Kube-scheduler高3.27% |
   | 通信延迟           | 0.648%     | 组件间Socket通信开销，占总Makespan比例极低 |
   | 调度延迟           | 38.2ms     | 含决策延迟（~1.5ms）与通信延迟（~35ms），Kube-scheduler为4.3ms |
   | Makespan比例       | 1.03-1.06× | 略高于Kube-scheduler，但延迟占比可忽略 |


## 四、相关工作与未来方向
1. **相关工作分类**  
   - Kubernetes调度优化：Stratus（批处理作业成本优化）、KCSS（多资源指标+TOPSIS算法）、NetMARKS（基于Istio的网络依赖感知）、KubeCG（GPU资源调度优化）；  
   - 强化学习调度：Decima（Spark集群GNN特征提取）、DeepRM（CNN处理资源状态图像）、RLSK（多集群DQN调度）。

2. **未来优化方向**  
   - 硬件感知增强：支持GPU、FPGA等新型硬件的资源监控与调度；  
   - 架构扩展：引入分布式调度、多层级调度，避免单点故障。


---


# 4. 关键问题
## 问题1：DRS相比Kubernetes原生调度器（Kube-scheduler），在资源感知维度和调度逻辑上有哪些核心差异？这些差异如何直接提升调度性能？
### 答案：
- **资源感知维度差异**：Kube-scheduler仅关注**CPU和内存**2项资源，DRS扩展至**6项指标**（CPU利用率、内存利用率、网络收包速率、网络发包速率、磁盘读速率、磁盘写速率），覆盖网络、磁盘等易被忽略的关键资源，可适配网络密集型、IO密集型应用；  
- **调度逻辑差异**：Kube-scheduler采用“过滤-固定规则评分（单节点CPU/内存均衡）”的静态逻辑，DRS基于**马尔可夫决策过程（MDP）** 建模，以集群全局的“平均资源利用率（AvgUtil）+节点负载不平衡度（Imbalance）”为奖励函数，通过**DQN算法自动学习调度策略**，无需依赖专家知识；  
- **性能提升关联**：多资源感知避免单一资源过载导致的负载不均，全局视角减少资源碎片化，自动学习策略适配动态工作负载，最终实现**27.29%资源利用率提升**和**2.90×负载不平衡度降低**。


## 问题2：DRS的核心组件（DRS monitor、DRS decision maker）如何协同工作完成一次Pod调度？整个流程中关键的时间参数和数据处理逻辑是什么？
### 答案：
- **协同流程**：  
  1. 用户提交Pod部署请求至Kubernetes主节点，DRS scheduler进入调度周期，先过滤掉资源不足的节点；  
  2. DRS scheduler向DRS decision maker发送调度请求（含Pod资源需求）；  
  3. DRS decision maker向所有工作节点的**DRS monitor**发送资源查询请求；  
  4. DRS monitor提取本地队列中存储的资源数据（每1秒采集，按“平均时间（AT）”取前N个数据计算平均值），返回6项资源指标；  
  5. DRS decision maker将节点资源数据与Pod需求整合为State向量，用ε-贪心策略（基于DQN模型）选择最优节点（Action），返回给DRS scheduler；  
  6. DRS scheduler完成Pod绑定，等待**Tᵢ=5秒**（确保Pod启动）后，DRS decision maker再次获取节点状态，计算Reward并将（Stateᵗ, Actionᵗ, Rewardᵗ, Stateᵗ⁺¹）存入经验池（容量N=300），池满后采样训练；  
- **关键参数与逻辑**：数据采集间隔**1秒**、Pod启动等待时间**5秒**、经验池容量**300**、目标网络参数更新间隔**50轮迭代**，DRS monitor用队列存储数据确保平均值准确性，DQN算法通过经验回放避免样本相关性影响训练。


## 问题3：DRS在实验中验证了3类工作负载（均匀、随机、CPU密集）的适配性，不同工作负载下DRS与对比算法（Kube-scheduler、Random、Round Robin）的性能表现有何差异？这种差异反映出各算法的适用场景局限是什么？
### 答案：
- **性能表现差异**：  
  | 工作负载类型 | DRS表现                          | Kube-scheduler表现                  | Random/Round Robin表现              |
  |--------------|----------------------------------|-------------------------------------|-------------------------------------|
  | 均匀负载     | 奖励值最高，资源利用率47.60%，不平衡度0.02 | 奖励值低，资源利用率33.60%，不平衡度0.08 | 奖励值中等，利用率37.5%-38.58%，不平衡度0.06 |
  | 随机负载     | 100轮后收敛，奖励值稳定高于对比算法 | 奖励值波动大，资源利用率33.21%      | 利用率37.46%-38.22%，波动较DRS大    |
  | CPU密集负载  | 奖励值略高于Kube-scheduler，利用率41.90% | 奖励值接近DRS（因关注CPU），利用率38.48% | 利用率33.88%-33.94%，CPU资源分配不均 |
- **算法适用场景局限**：  
  - Kube-scheduler：仅适用于**CPU密集型工作负载**，在需均衡利用网络、磁盘的场景中性能显著下降；  
  - Random/Round Robin：仅适用于**资源需求稳定且简单的场景**，随机特性导致负载波动大，复杂工作负载下利用率低；  
  - DRS：无明显场景局限，可适配动态、多类型工作负载，仅在CPU密集场景下相比Kube-scheduler优势略小，但仍保持更高利用率。